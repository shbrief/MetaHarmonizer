---
title: "Clustering embeddings for unsupervised schema mapping design"
author: "Sehyun Oh"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  BiocStyle::html_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(comment = "#>", 
                      collapse = TRUE, 
                      message = FALSE, 
                      warning = FALSE)
```


# Setup and data preparation
```{r}
library(cluster)
library(factoextra)
```

```{r}
embeddings <- as.matrix(read.csv("data/embeddings.csv", header = FALSE))
unique_values <- readLines("data/unique_values.txt")
rownames(embeddings) <- unique_values
```

```{r eval=FALSE}
# Assuming your embeddings are in a matrix called 'embeddings'
# with dimensions 673 x 768

# Normalize embeddings for cosine similarity
normalize_vectors <- function(mat) {
  norms <- sqrt(rowSums(mat^2))
  mat / norms
}

normalized_embeddings <- normalize_vectors(embeddings)
```

# Clustering
## K-means with cosine distance
```{r}
# K-Means on normalized embeddings
set.seed(42)
kmeans_result <- kmeans(embeddings, centers = 10, nstart = 25)
clusters_kmeans <- kmeans_result$cluster

# Find optimal k using elbow method
wss <- sapply(1:15, function(k) {
  kmeans(embeddings, centers = k, nstart = 25)$tot.withinss
})
plot(1:15, wss, type = "b", xlab = "Number of Clusters", ylab = "WSS",
     main = "Elbow Method for Optimal k")

# Visualize clusters using PCA (2D projection)
library(ggplot2)
pca_2d <- prcomp(embeddings, center = TRUE, scale. = FALSE)
pca_coords <- as.data.frame(pca_2d$x[, 1:2])
pca_coords$cluster <- as.factor(clusters_kmeans)

ggplot(pca_coords, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(size = 2, alpha = 0.6) +
  theme_minimal() +
  labs(title = "K-Means Clustering (PCA Projection)",
       x = "First Principal Component",
       y = "Second Principal Component") +
  theme(legend.position = "right")

# Alternative: Use factoextra for nice visualization
fviz_cluster(kmeans_result, data = embeddings,
             geom = "point",
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal(),
             main = "K-Means Clustering Visualization")

# Silhouette plot
sil <- silhouette(clusters_kmeans, dist(embeddings))
fviz_silhouette(sil)
```

Evaluate clusters
```{r}
# Silhouette score
library(cluster)
sil_score <- silhouette(clusters_kmeans, dist(embeddings))
mean(sil_score[, 3])

# Davies-Bouldin index (lower is better)
library(clusterSim)
index.DB(embeddings, clusters_kmeans)$DB
```


## Hierarchical clustering with cosine distance
```{r cosine_dist_func}
# Check your data type first
class(embeddings)

# Convert to numeric matrix if needed
embeddings <- as.matrix(embeddings)
embeddings <- apply(embeddings, 2, as.numeric)

# Improved cosine distance function with error checking
cosine_dist <- function(mat) {
  # Ensure it's a numeric matrix
  mat <- as.matrix(mat)
  mat <- apply(mat, 2, as.numeric)
  
  # Calculate norms
  norms <- sqrt(rowSums(mat^2))
  
  # Normalize (avoid division by zero)
  sim <- mat / norms
  
  # Calculate cosine similarity matrix
  sim_matrix <- sim %*% t(sim)
  
  # Convert to distance (1 - similarity)
  dist_matrix <- as.dist(1 - sim_matrix)
  
  return(dist_matrix)
}
```

```{r}
# Calculate cosine distance
dist_matrix <- cosine_dist(embeddings)

# Agglomerative clustering
hc <- hclust(dist_matrix, method = "average")
clusters_hc <- cutree(hc, k = 10)

# Visualize dendrogram
plot(hc, labels = FALSE, hang = -1, main = "Hierarchical Clustering Dendrogram")
rect.hclust(hc, k = 10, border = "red")

# Visualize clusters in 2D
pca_coords$cluster_hc <- as.factor(clusters_hc)
ggplot(pca_coords, aes(x = PC1, y = PC2, color = cluster_hc)) +
  geom_point(size = 2, alpha = 0.6) +
  theme_minimal() +
  labs(title = "Hierarchical Clustering (PCA Projection)",
       x = "First Principal Component",
       y = "Second Principal Component")

# Fancy dendrogram with factoextra
fviz_dend(hc, k = 10, 
          cex = 0.5,
          k_colors = "jco",
          color_labels_by_k = TRUE,
          rect = TRUE,
          main = "Hierarchical Clustering Dendrogram")
```


## HDBSCAN with cosine distance
```{r}
library(dbscan)

# HDBSCAN with cosine distance
dist_matrix <- cosine_dist(embeddings)
hdbscan_result <- hdbscan(dist_matrix, minPts = 5)
clusters_hdbscan <- hdbscan_result$cluster

# View results
table(clusters_hdbscan)  # 0 = noise points

# Plot clusters (excluding noise points)
pca_coords$cluster_hdbscan <- as.factor(clusters_hdbscan)
ggplot(pca_coords, aes(x = PC1, y = PC2, color = cluster_hdbscan, 
                       shape = cluster_hdbscan == 0)) +
  geom_point(size = 2, alpha = 0.6) +
  scale_shape_manual(values = c(19, 4), guide = "none") +  # X for noise
  theme_minimal() +
  labs(title = "HDBSCAN Clustering (PCA Projection)",
       subtitle = "Cluster 0 = Noise Points",
       x = "First Principal Component",
       y = "Second Principal Component")
```


## UMAP Visualization
```{r}
# UMAP (better for visualization)
library(umap)

# UMAP with 2 components
umap_result <- umap(embeddings, n_components = 2)
umap_coords <- as.data.frame(umap_result$layout)
colnames(umap_coords) <- c("UMAP1", "UMAP2")

# Plot K-Means clusters with UMAP
umap_coords$cluster <- as.factor(clusters_kmeans)
ggplot(umap_coords, aes(x = UMAP1, y = UMAP2, color = cluster)) +
  geom_point(size = 2, alpha = 0.6) +
  theme_minimal() +
  labs(title = "K-Means Clustering (UMAP Projection)",
       x = "UMAP Dimension 1",
       y = "UMAP Dimension 2")

# 3D UMAP visualization (optional)
library(plotly)
umap_3d <- umap(embeddings, n_components = 3)
umap_3d_coords <- as.data.frame(umap_3d$layout)
colnames(umap_3d_coords) <- c("UMAP1", "UMAP2", "UMAP3")
umap_3d_coords$cluster <- as.factor(clusters_kmeans)

plot_ly(umap_3d_coords, 
        x = ~UMAP1, y = ~UMAP2, z = ~UMAP3,
        color = ~cluster,
        type = "scatter3d",
        mode = "markers",
        marker = list(size = 3)) %>%
  layout(title = "3D UMAP Clustering Visualization")
```


# Evaluation
## Comparison plot
```{r}
# Compare all three methods
library(gridExtra)

p1 <- ggplot(pca_coords, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(size = 2, alpha = 0.6) +
  theme_minimal() +
  labs(title = "K-Means") +
  theme(legend.position = "none")

p2 <- ggplot(pca_coords, aes(x = PC1, y = PC2, color = cluster_hc)) +
  geom_point(size = 2, alpha = 0.6) +
  theme_minimal() +
  labs(title = "Hierarchical") +
  theme(legend.position = "none")

p3 <- ggplot(pca_coords, aes(x = PC1, y = PC2, color = cluster_hdbscan)) +
  geom_point(size = 2, alpha = 0.6) +
  theme_minimal() +
  labs(title = "HDBSCAN") +
  theme(legend.position = "none")

grid.arrange(p1, p2, p3, ncol = 3, 
             top = "Comparison of Clustering Methods")
```

## Interactive plot with hover information
```{r}
library(plotly)

# Add any metadata you have (e.g., text snippets)
pca_coords$text_label <- paste("Point:", 1:nrow(pca_coords))  # Replace with actual labels

plot_ly(pca_coords, 
        x = ~PC1, 
        y = ~PC2, 
        color = ~cluster,
        text = ~text_label,
        type = "scatter",
        mode = "markers",
        marker = list(size = 8)) %>%
  layout(title = "Interactive Cluster Visualization",
         xaxis = list(title = "PC1"),
         yaxis = list(title = "PC2"))
```


# Evaluation
## Evaluation for All Clustering Methods

```{r}
library(cluster)
library(clusterSim)
library(proxy)

# Calculate distance matrix once (using cosine distance)
dist_matrix <- dist(embeddings, method = "cosine")
# Or if using normalized embeddings with Euclidean:
# dist_matrix <- dist(embeddings)

# ----- K-Means Evaluation -----
kmeans_result <- kmeans(embeddings, centers = 10, nstart = 25)
clusters_kmeans <- kmeans_result$cluster

sil_kmeans <- silhouette(clusters_kmeans, dist_matrix)
db_kmeans <- index.DB(embeddings, clusters_kmeans)$DB

cat("K-Means Results:\n")
cat("Silhouette Score:", mean(sil_kmeans[, 3]), "\n")
cat("Davies-Bouldin Index:", db_kmeans, "\n\n")

# ----- Hierarchical Clustering Evaluation -----
hc <- hclust(dist_matrix, method = "average")
clusters_hc <- cutree(hc, k = 10)

sil_hc <- silhouette(clusters_hc, dist_matrix)
db_hc <- index.DB(embeddings, clusters_hc)$DB

cat("Hierarchical Clustering Results:\n")
cat("Silhouette Score:", mean(sil_hc[, 3]), "\n")
cat("Davies-Bouldin Index:", db_hc, "\n\n")

# ----- HDBSCAN Evaluation -----
library(dbscan)
hdbscan_result <- hdbscan(dist_matrix, minPts = 5)
clusters_hdbscan <- hdbscan_result$cluster

# IMPORTANT: Exclude noise points (cluster 0) for evaluation
non_noise <- clusters_hdbscan != 0

if (sum(non_noise) > 0 && length(unique(clusters_hdbscan[non_noise])) > 1) {
  # Filter out noise points
  clusters_filtered <- clusters_hdbscan[non_noise]
  embeddings_filtered <- embeddings[non_noise, ]
  dist_matrix_filtered <- dist(embeddings_filtered, method = "cosine")
  
  sil_hdbscan <- silhouette(clusters_filtered, dist_matrix_filtered)
  db_hdbscan <- index.DB(embeddings_filtered, clusters_filtered)$DB
  
  cat("HDBSCAN Results (excluding noise):\n")
  cat("Silhouette Score:", mean(sil_hdbscan[, 3]), "\n")
  cat("Davies-Bouldin Index:", db_hdbscan, "\n")
  cat("Number of noise points:", sum(!non_noise), "\n")
  cat("Number of clusters:", length(unique(clusters_filtered)), "\n\n")
} else {
  cat("HDBSCAN: Not enough valid clusters for evaluation\n\n")
}
```

```{r comprehensive_cluster_evaluation_function}
# Function to evaluate any clustering result
# Function to evaluate any clustering result
evaluate_clustering <- function(clusters, embeddings, dist_matrix = NULL, 
                                method_name = "Clustering") {
  
  # Remove noise points if present (cluster 0 or -1)
  valid_clusters <- clusters[clusters > 0 & clusters != -1]
  valid_indices <- clusters > 0 & clusters != -1
  
  # Check if we have enough clusters
  n_clusters <- length(unique(valid_clusters))
  
  if (n_clusters < 2) {
    cat(method_name, ": Not enough clusters for evaluation\n")
    return(NULL)
  }
  
  # Filter embeddings
  embeddings_filtered <- embeddings[valid_indices, ]
  
  # Calculate distance matrix if not provided
  if (is.null(dist_matrix)) {
    dist_matrix_filtered <- dist(embeddings_filtered, method = "cosine")
  } else {
    # Subset the distance matrix
    dist_matrix_filtered <- as.dist(as.matrix(dist_matrix)[valid_indices, valid_indices])
  }
  
  # Calculate metrics
  sil <- silhouette(valid_clusters, dist_matrix_filtered)
  sil_score <- mean(sil[, 3])
  
  db_score <- index.DB(embeddings_filtered, valid_clusters)$DB
  
  # Calinski-Harabasz index manually (if function not available)
  ch_score <- tryCatch({
    # Try using the package function first
    index.calinski(embeddings_filtered, valid_clusters)
  }, error = function(e) {
    # Calculate manually if function not available
    n <- nrow(embeddings_filtered)
    k <- n_clusters
    
    # Overall mean
    overall_mean <- colMeans(embeddings_filtered)
    
    # Between-cluster sum of squares
    BCSS <- 0
    for (i in unique(valid_clusters)) {
      cluster_points <- embeddings_filtered[valid_clusters == i, , drop = FALSE]
      n_i <- nrow(cluster_points)
      cluster_mean <- colMeans(cluster_points)
      BCSS <- BCSS + n_i * sum((cluster_mean - overall_mean)^2)
    }
    
    # Within-cluster sum of squares
    WCSS <- 0
    for (i in unique(valid_clusters)) {
      cluster_points <- embeddings_filtered[valid_clusters == i, , drop = FALSE]
      cluster_mean <- colMeans(cluster_points)
      WCSS <- WCSS + sum(apply(cluster_points, 1, function(x) sum((x - cluster_mean)^2)))
    }
    
    # Calinski-Harabasz index
    ch <- (BCSS / (k - 1)) / (WCSS / (n - k))
    return(ch)
  })
  
  # Print results
  cat("\n", method_name, "Evaluation:\n", sep = "")
  cat("----------------------------------------\n")
  cat("Number of clusters:", n_clusters, "\n")
  cat("Number of points:", length(valid_clusters), "\n")
  cat("Noise points:", sum(!valid_indices), "\n")
  cat("Silhouette Score:", round(sil_score, 4), 
      "(higher is better, range: -1 to 1)\n")
  cat("Davies-Bouldin Index:", round(db_score, 4), 
      "(lower is better)\n")
  cat("Calinski-Harabasz Index:", round(ch_score, 4), 
      "(higher is better)\n")
  
  # Return results as a list
  return(list(
    n_clusters = n_clusters,
    n_points = length(valid_clusters),
    noise_points = sum(!valid_indices),
    silhouette = sil_score,
    davies_bouldin = db_score,
    calinski_harabasz = ch_score,
    silhouette_object = sil
  ))
}

# Use the function for all methods
kmeans_eval <- evaluate_clustering(clusters_kmeans, embeddings, 
                                   dist_matrix, "K-Means")
hc_eval <- evaluate_clustering(clusters_hc, embeddings, 
                               dist_matrix, "Hierarchical")
hdbscan_eval <- evaluate_clustering(clusters_hdbscan, embeddings, 
                                   dist_matrix, "HDBSCAN")
```

## Compare all methods visually
```{r}
# Create comparison data frame
comparison <- data.frame(
  Method = c("K-Means", "Hierarchical", "HDBSCAN"),
  Silhouette = c(kmeans_eval$silhouette, 
                 hc_eval$silhouette, 
                 hdbscan_eval$silhouette),
  Davies_Bouldin = c(kmeans_eval$davies_bouldin, 
                     hc_eval$davies_bouldin, 
                     hdbscan_eval$davies_bouldin),
  N_Clusters = c(kmeans_eval$n_clusters,
                 hc_eval$n_clusters,
                 hdbscan_eval$n_clusters)
)

print(comparison)

# Plot comparison
library(ggplot2)
library(gridExtra)

p1 <- ggplot(comparison, aes(x = Method, y = Silhouette, fill = Method)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Silhouette Score (Higher is Better)") +
  theme(legend.position = "none")

p2 <- ggplot(comparison, aes(x = Method, y = Davies_Bouldin, fill = Method)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Davies-Bouldin Index (Lower is Better)") +
  theme(legend.position = "none")

grid.arrange(p1, p2, ncol = 2)
```

```{r}
# Plot silhouette for each method
par(mfrow = c(2, 2))

plot(sil_kmeans, main = "K-Means Silhouette Plot", 
     col = 1:10, border = NA)

plot(sil_hc, main = "Hierarchical Silhouette Plot", 
     col = 1:10, border = NA)

if (!is.null(hdbscan_eval)) {
  plot(hdbscan_eval$silhouette_object, 
       main = "HDBSCAN Silhouette Plot", 
       col = 1:length(unique(clusters_hdbscan[clusters_hdbscan != 0])), 
       border = NA)
}

par(mfrow = c(1, 1))
```

