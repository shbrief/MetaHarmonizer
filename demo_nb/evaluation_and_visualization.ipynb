{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8596d1d",
   "metadata": {},
   "source": [
    "Compare different thresholds and strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435138f5",
   "metadata": {},
   "source": [
   "Explanation:\n",
   "\n",
   "base_path: \n",
    "The file path that contains all the results\n",
    "\n",
    "\n",
    "filename: Follow this pattern:\n",
    "\n",
    "data/outputs/2025/large_corpus/1128/lm_syn_bodysite_top5_result_0.8.csv\n",
    "\n",
    "data/outputs/2025/large_corpus/1128/st_syn_disease_top5_result_0.9.csv\n",
    "\n",
    "data/outputs/2025/large_corpus/1128/st_syn_trt_result_0.7.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ca5059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lcc/projects/MetaHarmonizer\n"
     ]
    }
   ],
   "source": [
    "%cd /home/lcc/projects/MetaHarmonizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ebe685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.calc_stats import CalcStats\n",
    "\n",
    "calc = CalcStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc2b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from evaluation.calc_stats import CalcStats\n",
    "\n",
    "def analyze_results(strategy='st', base_path=\"data/outputs/2025/large_corpus/1128\"):\n",
    "    \"\"\"\n",
    "    Analyze and summarize results for a given strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    - strategy: 'lm' or 'st'\n",
    "    - base_path: Directory where result files are located\n",
    "    \"\"\"\n",
    "    calc = CalcStats()\n",
    "    \n",
    "    # Define thresholds to analyze\n",
    "    thresholds = ['0.7', '0.8', '0.9']\n",
    "    \n",
    "    # Define categories\n",
    "    categories = {\n",
    "        'bodysite': 'Body Site',\n",
    "        'disease': 'Disease', \n",
    "        'trt': 'Treatment'\n",
    "    }\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Iterate over each category and threshold\n",
    "    for cat_key, cat_name in categories.items():\n",
    "        for threshold in thresholds:\n",
    "            # Construct filename\n",
    "            if cat_key == 'trt':\n",
    "                filename = f\"{strategy}_syn_{cat_key}_result_{threshold}.csv\"\n",
    "            else:\n",
    "                filename = f\"{strategy}_syn_{cat_key}_top5_result_{threshold}.csv\"\n",
    "            \n",
    "            file_path = os.path.join(base_path, filename)\n",
    "            \n",
    "            # Check if file exists\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Warning: File not found - {filename}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing: {filename}\")\n",
    "            \n",
    "            try:\n",
    "                data = pd.read_csv(file_path)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                accuracy_df = calc.calc_accuracy(data)\n",
    "                \n",
    "                # Add metadata\n",
    "                accuracy_df['Category'] = cat_name\n",
    "                accuracy_df['Threshold'] = threshold\n",
    "                accuracy_df['Strategy'] = strategy.upper()\n",
    "                accuracy_df['File'] = filename\n",
    "                \n",
    "                all_results.append(accuracy_df)\n",
    "                \n",
    "                print(f\"  ✓ {cat_name} @ {threshold}: Top1={accuracy_df.iloc[0]['Accuracy']:.2f}%\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error processing {filename}: {e}\")\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"No results found!\")\n",
    "        return None, None\n",
    "    \n",
    "    combined_results = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Reorder columns\n",
    "    combined_results = combined_results[['Strategy', 'Category', 'Threshold', \n",
    "                                        'Accuracy Level', 'Accuracy', 'File']]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SUMMARY - {strategy.upper()} Strategy Results\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(combined_results[['Category', 'Threshold', 'Accuracy Level', 'Accuracy']].to_string(index=False))\n",
    "    \n",
    "    # Create comparison tables: Thresholds (0.7, 0.8, 0.9) vertically, Top1/3/5 horizontally\n",
    "    comparison_tables = {}\n",
    "    \n",
    "    for cat_name in categories.values():\n",
    "        cat_data = combined_results[combined_results['Category'] == cat_name]\n",
    "        \n",
    "        if cat_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # Create pivot table: rows=Threshold, columns=Accuracy Level\n",
    "        pivot = cat_data.pivot_table(\n",
    "            index='Threshold',\n",
    "            columns='Accuracy Level',\n",
    "            values='Accuracy',\n",
    "            aggfunc='first'\n",
    "        )\n",
    "        \n",
    "        # Ensure column order is Top 1, Top 3, Top 5\n",
    "        column_order = ['Top 1 Match', 'Top 3 Matches', 'Top 5 Matches']\n",
    "        pivot = pivot[[col for col in column_order if col in pivot.columns]]\n",
    "        \n",
    "        # Rename columns for simpler display\n",
    "        pivot.columns = ['Top 1', 'Top 3', 'Top 5']\n",
    "        \n",
    "        # Ensure threshold order\n",
    "        pivot = pivot.reindex(['0.7', '0.8', '0.9'])\n",
    "        \n",
    "        comparison_tables[cat_name] = pivot\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{cat_name} - Accuracy Comparison (Threshold × Top-K)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(pivot.to_string())\n",
    "    \n",
    "    return combined_results, comparison_tables\n",
    "\n",
    "\n",
    "def compare_strategies(strategies=['lm', 'st'], \n",
    "                       base_path=\"data/outputs/2025/large_corpus/1128\",\n",
    "                       save_results=False):\n",
    "    \"\"\"\n",
    "    Compare results of multiple strategies\n",
    "    \n",
    "    Parameters:\n",
    "    - strategies: List of strategies, e.g., ['lm', 'st']\n",
    "    - base_path: the directory where result files are located\n",
    "    - save_results: Whether to save results to CSV\n",
    "    \"\"\"\n",
    "    all_strategy_results = []\n",
    "    all_comparison_tables = {}\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"# Analyzing Strategy: {strategy.upper()}\")\n",
    "        print(f\"{'#'*80}\\n\")\n",
    "        \n",
    "        results, tables = analyze_results(strategy=strategy, base_path=base_path)\n",
    "        \n",
    "        if results is not None:\n",
    "            all_strategy_results.append(results)\n",
    "            all_comparison_tables[strategy] = tables\n",
    "    \n",
    "    if not all_strategy_results:\n",
    "        print(\"No results to compare!\")\n",
    "        return\n",
    "    \n",
    "    # Combine results from all strategies\n",
    "    final_combined = pd.concat(all_strategy_results, ignore_index=True)\n",
    "    \n",
    "    # Cross-strategy comparison tables\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(\"CROSS-STRATEGY COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    categories = final_combined['Category'].unique()\n",
    "    \n",
    "    for category in categories:\n",
    "        for acc_level in ['Top 1 Match', 'Top 3 Matches', 'Top 5 Matches']:\n",
    "            cat_level_data = final_combined[\n",
    "                (final_combined['Category'] == category) &\n",
    "                (final_combined['Accuracy Level'] == acc_level)\n",
    "            ]\n",
    "            \n",
    "            if cat_level_data.empty:\n",
    "                continue\n",
    "            \n",
    "            # Create strategy comparison pivot table\n",
    "            strategy_pivot = cat_level_data.pivot_table(\n",
    "                index='Threshold',\n",
    "                columns='Strategy',\n",
    "                values='Accuracy',\n",
    "                aggfunc='first'\n",
    "            )\n",
    "            \n",
    "            # Ensure threshold order\n",
    "            strategy_pivot = strategy_pivot.reindex(['0.7', '0.8', '0.9'])\n",
    "            \n",
    "            print(f\"\\n{category} - {acc_level}\")\n",
    "            print(\"-\" * 60)\n",
    "            print(strategy_pivot.to_string())\n",
    "    \n",
    "    # Save results\n",
    "    if save_results:\n",
    "        output_path = os.path.join(base_path, f\"accuracy_comparison_{'_'.join(strategies)}.csv\")\n",
    "        final_combined.to_csv(output_path, index=False)\n",
    "        print(f\"\\n\\nResults saved to: {output_path}\")\n",
    "        \n",
    "        # Save comparison tables for each category\n",
    "        for strategy, tables in all_comparison_tables.items():\n",
    "            for cat_name, table in tables.items():\n",
    "                filename = f\"{strategy}_{cat_name.replace(' ', '_').lower()}_comparison.csv\"\n",
    "                table_path = os.path.join(base_path, filename)\n",
    "                table.to_csv(table_path)\n",
    "                print(f\"Saved: {filename}\")\n",
    "    \n",
    "    return final_combined, all_comparison_tables\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a07941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single strategy analysis\n",
    "results_lm, tables_lm = analyze_results(strategy='st')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3951da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test two strategies comparison\n",
    "combined, all_tables = compare_strategies(\n",
    "        strategies=['lm', 'st'], \n",
    "        save_results=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd086c",
   "metadata": {},
   "source": [
    "Compare 2 strategies for threshold=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c487bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing strategy: LM\n",
      "  Processing: lm_syn_bodysite_top5_result_0.9.csv\n",
      "    ✓ bodysite: Top1=57.63%, Top3=79.73%, Top5=84.51%\n",
      "  Processing: lm_syn_disease_top5_result_0.9.csv\n",
      "    ✓ disease: Top1=80.64%, Top3=89.39%, Top5=91.64%\n",
      "  Processing: lm_syn_trt_result_0.9.csv\n",
      "    ✓ treatment: Top1=79.76%, Top3=85.64%, Top5=87.89%\n",
      "\n",
      "Processing strategy: ST\n",
      "  Processing: st_syn_bodysite_top5_result_0.9.csv\n",
      "    ✓ bodysite: Top1=64.46%, Top3=80.64%, Top5=86.10%\n",
      "  Processing: st_syn_disease_top5_result_0.9.csv\n",
      "    ✓ disease: Top1=82.32%, Top3=90.35%, Top5=92.60%\n",
      "  Processing: st_syn_trt_result_0.9.csv\n",
      "    ✓ treatment: Top1=81.31%, Top3=85.29%, Top5=87.02%\n",
      "\n",
      "================================================================================\n",
      "RESULTS SUMMARY (Threshold = 0.9)\n",
      "================================================================================\n",
      "\n",
      "Attribute Strategy  Top 1 (%)  Top 3 (%)  Top 5 (%)\n",
      " bodysite       LM      57.63      79.73      84.51\n",
      " bodysite       ST      64.46      80.64      86.10\n",
      "  disease       LM      80.64      89.39      91.64\n",
      "  disease       ST      82.32      90.35      92.60\n",
      "treatment       LM      79.76      85.64      87.89\n",
      "treatment       ST      81.31      85.29      87.02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from evaluation.calc_stats import CalcStats\n",
    "\n",
    "def analyze_results_formatted(strategies=['lm', 'st'], \n",
    "                              base_path=\"data/outputs/2025/large_corpus/1128\",\n",
    "                              threshold='0.9',\n",
    "                              save_results=False):\n",
    "    \"\"\"\n",
    "    Analyze and summarize results for given strategies in a formatted table.\n",
    "    Only uses SapBERT model.\n",
    "    \"\"\"\n",
    "    calc = CalcStats()\n",
    "    \n",
    "    # Define categories\n",
    "    categories = {\n",
    "        'bodysite': 'bodysite',\n",
    "        'disease': 'disease', \n",
    "        'trt': 'treatment'\n",
    "    }\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Iterate over each strategy\n",
    "    for strategy in strategies:\n",
    "        print(f\"\\nProcessing strategy: {strategy.upper()}\")\n",
    "        \n",
    "        for cat_key, cat_name in categories.items():\n",
    "            # Use strategy as the file prefix (lm or st)\n",
    "            strat_code = strategy.lower()\n",
    "            \n",
    "            # Construct filename\n",
    "            if cat_key == 'trt':\n",
    "                filename = f\"{strat_code}_syn_{cat_key}_result_{threshold}.csv\"\n",
    "            else:\n",
    "                filename = f\"{strat_code}_syn_{cat_key}_top5_result_{threshold}.csv\"\n",
    "            \n",
    "            file_path = os.path.join(base_path, filename)\n",
    "            \n",
    "            # Check if file exists\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"  Warning: File not found - {filename}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Processing: {filename}\")\n",
    "            \n",
    "            try:\n",
    "                data = pd.read_csv(file_path)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                accuracy_df = calc.calc_accuracy(data)\n",
    "                \n",
    "                # Extract Top 1, Top 3, Top 5 accuracies\n",
    "                top1 = accuracy_df[accuracy_df['Accuracy Level'] == 'Top 1 Match']['Accuracy'].values[0]\n",
    "                top3 = accuracy_df[accuracy_df['Accuracy Level'] == 'Top 3 Matches']['Accuracy'].values[0]\n",
    "                top5 = accuracy_df[accuracy_df['Accuracy Level'] == 'Top 5 Matches']['Accuracy'].values[0]\n",
    "                \n",
    "                # Add to results\n",
    "                result_row = {\n",
    "                    'Attribute': cat_name,\n",
    "                    'Strategy': strategy.upper(),\n",
    "                    'Top 1 (%)': round(top1, 2),\n",
    "                    'Top 3 (%)': round(top3, 2),\n",
    "                    'Top 5 (%)': round(top5, 2)\n",
    "                }\n",
    "                \n",
    "                all_results.append(result_row)\n",
    "                \n",
    "                print(f\"    ✓ {cat_name}: Top1={top1:.2f}%, Top3={top3:.2f}%, Top5={top5:.2f}%\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ✗ Error processing {filename}: {e}\")\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"No results found!\")\n",
    "        return None\n",
    "    \n",
    "    # Create DataFrame\n",
    "    final_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Sort by Attribute, Strategy\n",
    "    final_df = final_df.sort_values(['Attribute', 'Strategy'])\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RESULTS SUMMARY (Threshold = {threshold})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(final_df.to_string(index=False))\n",
    "    \n",
    "    # Save results\n",
    "    if save_results:\n",
    "        output_path = os.path.join(base_path, f\"accuracy_summary_threshold_{threshold}.csv\")\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\n\\nResults saved to: {output_path}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Analyze LM and ST strategies, only for threshold=0.9\n",
    "    results = analyze_results_formatted(\n",
    "        strategies=['lm', 'st'],\n",
    "        base_path=\"data/outputs/2025/large_corpus/1128\",\n",
    "        threshold='0.9',\n",
    "        save_results=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bcbfc7",
   "metadata": {},
   "source": [
    "The graphs are generated by manually adding results; Need improve (automatically generate graph from evaluation results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafc184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Top-K\n",
    "x = [1, 3, 5]\n",
    "\n",
    "# ---- Previous\n",
    "prev = {\n",
    "    \"Body Site\": [58.997722, 75.398633, 82.460137],\n",
    "    \"Disease\":   [75.562701, 84.694534, 87.781350],\n",
    "    \"Treatment\": [61.764706, 67.301038, 69.723183],\n",
    "}\n",
    "\n",
    "# ---- Current\n",
    "curr = {\n",
    "    \"Body Site\": [64.46, 80.64, 86.10],\n",
    "    \"Disease\":   [82.32, 90.35, 92.60],\n",
    "    \"Treatment\": [81.31, 85.29, 87.02],\n",
    "}\n",
    "\n",
    "\n",
    "colors = {\n",
    "    \"Body Site\": \"#1f77b4\",   \n",
    "    \"Disease\":   \"#2ca02c\",   \n",
    "    \"Treatment\": \"#d62728\",   \n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for category in prev.keys():\n",
    "    color = colors[category]\n",
    "\n",
    "    # previous — dotted line\n",
    "    plt.plot(\n",
    "        x, prev[category],\n",
    "        marker='o',\n",
    "        linestyle='--',\n",
    "        color=color,\n",
    "        label=f\"{category} — Previous\"\n",
    "    )\n",
    "\n",
    "    # current — solid line\n",
    "    plt.plot(\n",
    "        x, curr[category],\n",
    "        marker='o',\n",
    "        linestyle='-',\n",
    "        linewidth=2,\n",
    "        color=color,\n",
    "        label=f\"{category} — Current\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Top-K\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Previous vs Current Accuracy (Top-1 / Top-3 / Top-5)\")\n",
    "plt.xticks([1, 3, 5])\n",
    "plt.ylim(55, 100)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456a055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
